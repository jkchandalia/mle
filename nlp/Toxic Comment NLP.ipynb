{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this Notebook\n",
    "\n",
    "The goal of this notebook is to build a classifier to find toxic comments. The data has been taken from a series of Kaggle competitions to classify Wikipedia comments as toxic/nontoxic. The data has been sourced from Google and Jigsaw. \n",
    "\n",
    "The notebook will start with simple bag-of-words and tf-idf features and use simple models like logistic regression and Naive Bayes to perform classification with these features. Though the full dataset includes non-English comments, I will restrict myself to English-only comment for this iteration. \n",
    "\n",
    "We will then move on to deep learning approaches, using a combination of pretrained word embeddings and simple deep learning models like RNNs and 1D convolutions to do more benchmarking. \n",
    "\n",
    "Next, we will explore deep learning models that have 'memory' using LSTMs (Long Short Term Memory) and GRUs (Gated Recurrent Units). \n",
    "\n",
    "Finally, we will approach state of the art performance using pretrained models like BERT and xlnet.\n",
    "\n",
    "For metrics, I will focus more on recall than precision as I believe finding all toxic comments is more important that mislabelling some nontoxic comments. In addition to recall, I will look at the confusion matrix and precision-recall curves. \n",
    "\n",
    "Credits:\n",
    "https://www.kaggle.com/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv')\n",
    "#The following is a non-English dataset and won't be used presently\n",
    "validation = pd.read_csv('jigsaw-multilingual-toxic-comment-classification/validation.csv')\n",
    "#The following is a non-English dataset and won't be used presently\n",
    "test = pd.read_csv('jigsaw-multilingual-toxic-comment-classification/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 223549 entries, 0 to 223548\n",
      "Data columns (total 8 columns):\n",
      "id               223549 non-null object\n",
      "comment_text     223549 non-null object\n",
      "toxic            223549 non-null int64\n",
      "severe_toxic     223549 non-null int64\n",
      "obscene          223549 non-null int64\n",
      "threat           223549 non-null int64\n",
      "insult           223549 non-null int64\n",
      "identity_hate    223549 non-null int64\n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 13.6+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop the other columns and approach this problem as a Binary Classification Problem and also we will have our exercise done on a smaller subsection of the dataset(only 12000 data points) to make it easier to train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Hey... what is it..\\n@ | talk .\\nWhat is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI-SOCIAL and DESTRUCTIVE (non)-contribution at WP?\\n\\nAsk Sityush to clean up his behavior than issue me nonsensical warnings...'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.loc[:10000,:]\n",
    "train.comment_text[train.toxic==1][1:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.comment_text.values, train.toxic.values, \n",
    "                                                  stratify=train.toxic.values, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)\n",
    "\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_train = count_vectorizer.fit_transform(xtrain)\n",
    "count_valid = count_vectorizer.transform(xvalid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13360"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(count_train.A[:5]))\n",
    "count_vectorizer.vocabulary_['hello']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29, 75,  5, ...,  1,  1,  1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(count_train.A,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1798    9]\n",
      " [ 103   91]]\n",
      "Accuracy Score: 0.94\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "nb_classifier.fit(count_train, ytrain)\n",
    "pred = nb_classifier.predict(count_valid)\n",
    "pred_proba = nb_classifier.predict_proba(count_valid)\n",
    "accuracy = metrics.accuracy_score(yvalid, pred)\n",
    "print(metrics.confusion_matrix(yvalid, pred, labels=[0,1]))\n",
    "print(\"Accuracy Score: {0:0.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision-recall score: 0.48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4783299329716585"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "average_precision = metrics.average_precision_score(yvalid, pred)\n",
    "#average_recall = metrics.recall_score(yvalid, pred)\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "\n",
    "\n",
    "disp = metrics.plot_precision_recall_curve(nb_classifier, count_valid, yvalid)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                   'AP={0:0.2f}'.format(average_precision))\n",
    "average_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Tfidf for the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "count_train_idf = tfidf_vectorizer.fit_transform(xtrain)\n",
    "count_valid_idf = tfidf_vectorizer.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5753045144327993"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(count_train_idf.A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(yvalid, pred, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1807    0]\n",
      " [ 181   13]]\n",
      "Accuracy Score: 0.91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'2-class Precision-Recall curve: AP=0.16')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_classifier.fit(count_train_idf, ytrain)\n",
    "pred = nb_classifier.predict(count_valid_idf)\n",
    "pred_proba = nb_classifier.predict_proba(count_valid_idf)\n",
    "accuracy = metrics.accuracy_score(yvalid, pred)\n",
    "print(metrics.confusion_matrix(yvalid, pred, labels=[0,1]))\n",
    "print(\"Accuracy Score: {0:0.2f}\".format(accuracy))\n",
    "\n",
    "average_precision = metrics.average_precision_score(yvalid, pred)\n",
    "\n",
    "disp = metrics.plot_precision_recall_curve(nb_classifier, count_valid, yvalid)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                   'AP={0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "In this Notebook I will start with the very Basics of RNN's and Build all the way to latest deep learning architectures to solve NLP problems. It will cover the Following:\n",
    "* Simple RNN's\n",
    "* Word Embeddings : Definition and How to get them\n",
    "* LSTM's\n",
    "* GRU's\n",
    "* BI-Directional RNN's\n",
    "* Encoder-Decoder Models (Seq2Seq Models)\n",
    "* Attention Models\n",
    "* Transformers - Attention is all you need\n",
    "* BERT\n",
    "\n",
    "I will divide every Topic into four subsections:\n",
    "* Basic Overview\n",
    "* In-Depth Understanding : In this I will attach links of articles and videos to learn about the topic in depth\n",
    "* Code-Implementation\n",
    "* Code Explanation\n",
    "\n",
    "This is a comprehensive kernel and if you follow along till the end , I promise you would learn all the techniques completely\n",
    "\n",
    "Note that the aim of this notebook is not to have a High LB score but to present a beginner guide to understand Deep Learning techniques used for NLP. Also after discussing all of these ideas , I will present a starter solution for this competiton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.comment_text.values, train.toxic.values, \n",
    "                                                  stratify=train.toxic.values, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU,SimpleRNN\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring TPU's\n",
    "\n",
    "For this version of Notebook we will be using TPU's as we have to built a BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before We Begin\n",
    "\n",
    "Before we Begin If you are a complete starter with NLP and never worked with text data, I am attaching a few kernels that will serve as a starting point of your journey\n",
    "* https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial\n",
    "* https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle\n",
    "\n",
    "If you want a more basic dataset to practice with here is another kernel which I wrote:\n",
    "* https://www.kaggle.com/tanulsingh077/what-s-cooking\n",
    "\n",
    "Below are some Resources to get started with basic level Neural Networks, It will help us to easily understand the upcoming parts\n",
    "* https://www.youtube.com/watch?v=aircAruvnKk&list=PL_h2yd2CGtBHEKwEH5iqTZH85wLS-eUzv\n",
    "* https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PL_h2yd2CGtBHEKwEH5iqTZH85wLS-eUzv&index=2\n",
    "* https://www.youtube.com/watch?v=Ilg3gGewQ5U&list=PL_h2yd2CGtBHEKwEH5iqTZH85wLS-eUzv&index=3\n",
    "* https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PL_h2yd2CGtBHEKwEH5iqTZH85wLS-eUzv&index=4\n",
    "\n",
    "For Learning how to visualize test data and what to use view:\n",
    "* https://www.kaggle.com/tanulsingh077/twitter-sentiment-extaction-analysis-eda-and-model\n",
    "* https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN\n",
    "\n",
    "## Basic Overview\n",
    "\n",
    "What is a RNN?\n",
    "\n",
    "Recurrent Neural Network(RNN) are a type of Neural Network where the output from previous step are fed as input to the current step. In traditional neural networks, all the inputs and outputs are independent of each other, but in cases like when it is required to predict the next word of a sentence, the previous words are required and hence there is a need to remember the previous words. Thus RNN came into existence, which solved this issue with the help of a Hidden Layer.\n",
    "\n",
    "Why RNN's?\n",
    "\n",
    "https://www.quora.com/Why-do-we-use-an-RNN-instead-of-a-simple-neural-network\n",
    "\n",
    "## In-Depth Understanding\n",
    "\n",
    "* https://medium.com/mindorks/understanding-the-recurrent-neural-network-44d593f112a2\n",
    "* https://www.youtube.com/watch?v=2E65LDnM2cA&list=PL1F3ABbhcqa3BBWo170U4Ev2wfsF7FN8l\n",
    "* https://www.d2l.ai/chapter_recurrent-neural-networks/rnn.html\n",
    "\n",
    "## Code Implementation\n",
    "\n",
    "So first I will implement the and then I will explain the code step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will check the maximum number of words that can be present in a comment , this will help us in padding later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text'].apply(lambda x:len(str(x).split())).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a function for getting auc score for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(predictions,target):\n",
    "    '''\n",
    "    This methods returns the AUC Score when given the Predictions\n",
    "    and Labels\n",
    "    '''\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return roc_auc, fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain[0]\n",
    "token = text.Tokenizer(num_words=None)\n",
    "token\n",
    "max_len = 1500\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xvalid_seq)\n",
    "out = token.texts_to_sequences(['fuck'])\n",
    "\n",
    "token.sequences_to_texts([[86]])\n",
    "word_index['fucker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 1500\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "#zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(ytrain))\n",
    "ybaseline = [True if 86 in seq else False for seq in xtrain_pad]\n",
    "sum(ybaseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    # A simpleRNN without any pretrained embeddings and one dense layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     input_length=max_len))\n",
    "    model.add(SimpleRNN(100))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xtrain_pad, ytrain, nb_epoch=2, batch_size=64*strategy.num_replicas_in_sync) #Multiplying by Strategy to run on TPU's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(xvalid_pad)\n",
    "score, fpr, tpr = roc_auc(scores,yvalid)\n",
    "print(\"Auc: %.2f%%\" % (score))\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(xtrain_pad)\n",
    "score, fpr, tpr = roc_auc(scores,ytrain)\n",
    "print(\"Auc: %.2f%%\" % (score))\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_model = []\n",
    "scores_model.append({'Model': 'SimpleRNN','AUC_Score': roc_auc(scores,yvalid)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Explanantion\n",
    "* Tokenization<br><br>\n",
    " So if you have watched the videos and referred to the links, you would know that in an RNN we input a sentence word by word. We represent every word as one hot vectors of dimensions : Numbers of words in Vocab +1. <br>\n",
    "  What keras Tokenizer does is , it takes all the unique words in the corpus,forms a dictionary with words as keys and their number of occurences as values,it then sorts the dictionary in descending order of counts. It then assigns the first value 1 , second value 2 and so on. So let's suppose word 'the' occured the most in the corpus then it will assigned index 1 and vector representing 'the' would be a one-hot vector with value 1 at position 1 and rest zereos.<br>\n",
    "  Try printing first 2 elements of xtrain_seq you will see every word is represented as a digit now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_seq[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now you might be wondering What is padding? Why its done</b><br><br>\n",
    "\n",
    "Here is the answer :\n",
    "* https://www.quora.com/Which-effect-does-sequence-padding-have-on-the-training-of-a-neural-network\n",
    "* https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/\n",
    "* https://www.coursera.org/lecture/natural-language-processing-tensorflow/padding-2Cyzs\n",
    "\n",
    "Also sometimes people might use special tokens while tokenizing like EOS(end of string) and BOS(Begining of string). Here is the reason why it's done\n",
    "* https://stackoverflow.com/questions/44579161/why-do-we-do-padding-in-nlp-tasks\n",
    "\n",
    "\n",
    "The code token.word_index simply gives the dictionary of vocab that keras created for us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Building the Neural Network\n",
    "\n",
    "To understand the Dimensions of input and output given to RNN in keras her is a beautiful article : https://medium.com/@shivajbd/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e\n",
    "\n",
    "The first line model.Sequential() tells keras that we will be building our network sequentially . Then we first add the Embedding layer.\n",
    "Embedding layer is also a layer of neurons which takes in as input the nth dimensional one hot vector of every word and converts it into 300 dimensional vector , it gives us word embeddings similar to word2vec. We could have used word2vec but the embeddings layer learns during training to enhance the embeddings.\n",
    "Next we add an 100 LSTM units without any dropout or regularization\n",
    "At last we add a single neuron with sigmoid function which takes output from 100 LSTM cells (Please note we have 100 LSTM cells not layers) to predict the results and then we compile the model using adam optimizer \n",
    "\n",
    "* Comments on the model<br><br>\n",
    "We can see our model achieves an accuracy of 1 which is just insane , we are clearly overfitting I know , but this was the simplest model of all ,we can tune a lot of hyperparameters like RNN units, we can do batch normalization , dropouts etc to get better result. The point is we got an AUC score of 0.82 without much efforts and we know have learnt about RNN's .Deep learning is really revolutionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "While building our simple RNN models we talked about using word-embeddings , So what is word-embeddings and how do we get word-embeddings?\n",
    "Here is the answer :\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/6Oq70/word-representation\n",
    "* https://machinelearningmastery.com/what-are-word-embeddings/\n",
    "<br> <br>\n",
    "The latest approach to getting word Embeddings is using pretained GLoVe or using Fasttext. Without going into too much details, I would explain how to create sentence vectors and how can we use them to create a machine learning model on top of it and since I am a fan of GloVe vectors, word2vec and fasttext. In this Notebook, I'll be using the GloVe vectors. You can download the GloVe vectors from here http://www-nlp.stanford.edu/data/glove.840B.300d.zip or you can search for GloVe in datasets on Kaggle and add the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('/kaggle/input/glove840b300dtxt/glove.840B.300d.txt','r',encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray([float(val) for val in values[1:]])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM's\n",
    "\n",
    "## Basic Overview\n",
    "\n",
    "Simple RNN's were certainly better than classical ML algorithms and gave state of the art results, but it failed to capture long term dependencies that is present in sentences . So in 1998-99 LSTM's were introduced to counter to these drawbacks.\n",
    "\n",
    "## In Depth Understanding\n",
    "\n",
    "Why LSTM's?\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/PKMRR/vanishing-gradients-with-rnns\n",
    "* https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/\n",
    "\n",
    "What are LSTM's?\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/KXoay/long-short-term-memory-lstm\n",
    "* https://distill.pub/2019/memorization-in-rnns/\n",
    "* https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "\n",
    "# Code Implementation\n",
    "\n",
    "We have already tokenized and paded our text for input to LSTM's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    \n",
    "    # A simple LSTM with glove embeddings and one dense layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "\n",
    "    model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xtrain_pad, ytrain, nb_epoch=5, batch_size=64*strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(xvalid_pad)\n",
    "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_model.append({'Model': 'LSTM','AUC_Score': roc_auc(scores,yvalid)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Explanation\n",
    "\n",
    "As a first step we calculate embedding matrix for our vocabulary from the pretrained GLoVe vectors . Then while building the embedding layer we pass Embedding Matrix as weights to the layer instead of training it over Vocabulary and thus we pass trainable = False.\n",
    "Rest of the model is same as before except we have replaced the SimpleRNN By LSTM Units\n",
    "\n",
    "* Comments on the Model\n",
    "\n",
    "We now see that the model is not overfitting and achieves an auc score of 0.96 which is quite commendable , also we close in on the gap between accuracy and auc .\n",
    "We see that in this case we used dropout and prevented overfitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU's\n",
    "\n",
    "## Basic  Overview\n",
    "\n",
    "Introduced by Cho, et al. in 2014, GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network. GRU's are a variation on the LSTM because both are designed similarly and, in some cases, produce equally excellent results . GRU's were designed to be simpler and faster than LSTM's and in most cases produce equally good results and thus there is no clear winner.\n",
    "\n",
    "## In Depth Explanation\n",
    "\n",
    "* https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/agZiL/gated-recurrent-unit-gru\n",
    "* https://www.geeksforgeeks.org/gated-recurrent-unit-networks/\n",
    "\n",
    "## Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    # GRU with glove embeddings and two dense layers\n",
    "     model = Sequential()\n",
    "     model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "     model.add(SpatialDropout1D(0.3))\n",
    "     model.add(GRU(300))\n",
    "     model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "     model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])   \n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xtrain_pad, ytrain, nb_epoch=5, batch_size=64*strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(xvalid_pad)\n",
    "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_model.append({'Model': 'GRU','AUC_Score': roc_auc(scores,yvalid)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Directional RNN's\n",
    "\n",
    "## In Depth Explanation\n",
    "\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/fyXnn/bidirectional-rnn\n",
    "* https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66\n",
    "* https://d2l.ai/chapter_recurrent-modern/bi-rnn.html\n",
    "\n",
    "## Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    # A simple bidirectional LSTM with glove embeddings and one dense layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "    model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xtrain_pad, ytrain, nb_epoch=5, batch_size=64*strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(xvalid_pad)\n",
    "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_model.append({'Model': 'Bi-directional LSTM','AUC_Score': roc_auc(scores,yvalid)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Explanation\n",
    "\n",
    "Code is same as before,only we have added bidirectional nature to the LSTM cells we used before and is self explanatory. We have achieve similar accuracy and auc score as before and now we have learned all the types of typical RNN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We are now at the end of part 1 of this notebook and things are about to go wild now as we Enter more complex and State of the art models .If you have followed along from the starting and read all the articles and understood everything , these complex models would be fairly easy to understand.I recommend Finishing Part 1 before continuing as the upcoming techniques can be quite overwhelming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Model Architecture\n",
    "\n",
    "## Overview\n",
    "\n",
    "RNN's are of many types  and different architectures are used for different purposes. Here is a nice video explanining different types of model architectures : https://www.coursera.org/learn/nlp-sequence-models/lecture/BO8PS/different-types-of-rnns.\n",
    "Seq2Seq is a many to many RNN architecture where the input is a sequence and the output is also a sequence (where input and output sequences can be or cannot be of different lengths). This architecture is used in a lot of applications like Machine Translation, text summarization, question answering etc\n",
    "\n",
    "## In Depth Understanding\n",
    "\n",
    "I will not write the code implementation for this,but rather I will provide the resources where code has already been implemented and explained in a much better way than I could have ever explained.\n",
    "\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/HyEui/basic-models ---> A basic idea of different Seq2Seq Models\n",
    "\n",
    "* https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html , https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/ ---> Basic Encoder-Decoder Model and its explanation respectively\n",
    "\n",
    "* https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639 ---> A More advanced Seq2seq Model and its explanation\n",
    "\n",
    "* https://d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html , https://d2l.ai/chapter_recurrent-modern/encoder-decoder.html ---> Implementation of Encoder-Decoder Model from scratch\n",
    "\n",
    "* https://www.youtube.com/watch?v=IfsjMg4fLWQ&list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9&index=8&t=0s ---> Introduction to Seq2seq By fast.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Results obtained from various Deep learning models\n",
    "results = pd.DataFrame(scores_model).sort_values(by='AUC_Score',ascending=False)\n",
    "results.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(go.Funnelarea(\n",
    "    text =results.Model,\n",
    "    values = results.AUC_Score,\n",
    "    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"}\n",
    "    ))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Models\n",
    "\n",
    "This is the toughest and most tricky part. If you are able to understand the intiuition and working of attention block , understanding transformers and transformer based architectures like BERT will be a piece of cake. This is the part where I spent the most time on and I suggest you do the same . Please read and view the following resources in the order I am providing to ignore getting confused, also at the end of this try to write and draw an attention block in your own way :-\n",
    "\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/RDXpX/attention-model-intuition --> Only watch this video and not the next one\n",
    "* https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a\n",
    "* https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc\n",
    "* https://distill.pub/2016/augmented-rnns/ \n",
    "\n",
    "## Code Implementation\n",
    "\n",
    "* https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/ --> Basic Level\n",
    "* https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html ---> Implementation from Scratch in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers : Attention is all you need\n",
    "\n",
    "So finally we have reached the end of the learning curve and are about to start learning the technology that changed NLP completely and are the reasons for the state of the art NLP techniques .Transformers were introduced in the paper Attention is all you need by Google. If you have understood the Attention models,this will be very easy , Here is transformers fully explained:\n",
    "\n",
    "* http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "## Code Implementation\n",
    "\n",
    "* http://nlp.seas.harvard.edu/2018/04/03/attention.html ---> This presents the code implementation of the architecture presented in the paper by Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT and Its Implementation on this Competition\n",
    "\n",
    "As Promised I am back with Resiurces , to understand about BERT architecture , please follow the contents in the given order :-\n",
    "\n",
    "* http://jalammar.github.io/illustrated-bert/ ---> In Depth Understanding of BERT\n",
    "\n",
    "After going through the post Above , I guess you must have understood how transformer architecture have been utilized by the current SOTA models . Now these architectures can be used in two ways :<br><br>\n",
    "1) We can use the model for prediction on our problems using the pretrained weights without fine-tuning or training the model for our sepcific tasks\n",
    "* EG: http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/ ---> Using Pre-trained BERT without Tuning\n",
    "\n",
    "2) We can fine-tune or train these transformer models for our task by tweaking the already pre-trained weights and training on a much smaller dataset\n",
    "* EG:* https://www.youtube.com/watch?v=hinZO--TEk4&t=2933s ---> Tuning BERT For your TASK\n",
    "\n",
    "We will be using the first example as a base for our implementation of BERT model using Hugging Face and KERAS , but contrary to first example we will also Fine-Tune our model for our task\n",
    "\n",
    "Acknowledgements : https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n",
    "\n",
    "\n",
    "Steps Involved :\n",
    "* Data Preparation : Tokenization and encoding of data\n",
    "* Configuring TPU's \n",
    "* Building a Function for Model Training and adding an output layer for classification\n",
    "* Train the model and get the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Dependencies\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import transformers\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING THE DATA\n",
    "\n",
    "train1 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n",
    "valid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\n",
    "test = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')\n",
    "sub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder FOr DATA for understanding waht encode batch does read documentation of hugging face tokenizer :\n",
    "https://huggingface.co/transformers/main_classes/tokenizer.html here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n",
    "    \"\"\"\n",
    "    Encoder for encoding the text into sequence of integers for BERT Input\n",
    "    \"\"\"\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding(max_length=maxlen)\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "    \n",
    "    return np.array(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMP DATA FOR CONFIG\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "MAX_LEN = 192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "For understanding please refer to hugging face documentation again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the real tokenizer\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "# Save the loaded tokenizer locally\n",
    "tokenizer.save_pretrained('.')\n",
    "# Reload it with the huggingface tokenizers library\n",
    "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = fast_encode(train1.comment_text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "x_valid = fast_encode(valid.comment_text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "x_test = fast_encode(test.content.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = train1.toxic.values\n",
    "y_valid = valid.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_train, y_train))\n",
    "    .repeat()\n",
    "    .shuffle(2048)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_valid, y_valid))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(x_test)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, max_len=512):\n",
    "    \"\"\"\n",
    "    function for training the BERT model\n",
    "    \"\"\"\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(cls_token)\n",
    "    \n",
    "    model = Model(inputs=input_word_ids, outputs=out)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Training\n",
    "\n",
    "If you want to use any another model just replace the model name in transformers._____ and use accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    transformer_layer = (\n",
    "        transformers.TFDistilBertModel\n",
    "        .from_pretrained('distilbert-base-multilingual-cased')\n",
    "    )\n",
    "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = x_train.shape[0] // BATCH_SIZE\n",
    "train_history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=n_steps,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = x_valid.shape[0] // BATCH_SIZE\n",
    "train_history_2 = model.fit(\n",
    "    valid_dataset.repeat(),\n",
    "    steps_per_epoch=n_steps,\n",
    "    epochs=EPOCHS*2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['toxic'] = model.predict(test_dataset, verbose=1)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Notes\n",
    "\n",
    "This was my effort to share my learnings so that everyone can benifit from it.As this community has been very kind to me and helped me in learning all of this , I want to take this forward. I have shared all the resources I used to learn all the stuff .Join me and make these NLP competitions your first ,without being overwhelmed by the shear number of techniques used . It took me 10 days to learn all of this , you can learn it at your pace and dont give in , at the end of all this you will be a different person and it will all be worth it.\n",
    "\n",
    "\n",
    "### I am attaching more resources if you want NLP end to end:\n",
    "\n",
    "1) Books\n",
    "\n",
    "* https://d2l.ai/\n",
    "* Jason Brownlee's Books\n",
    "\n",
    "2) Courses\n",
    "\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/home/welcome\n",
    "* Fast.ai NLP Course\n",
    "\n",
    "3) Blogs and websites\n",
    "\n",
    "* Machine Learning Mastery\n",
    "* https://distill.pub/\n",
    "* http://jalammar.github.io/\n",
    "\n",
    "**<span style=\"color:Red\">This is subtle effort of contributing towards the community, if it helped you in any way please show a token of love by upvoting**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
